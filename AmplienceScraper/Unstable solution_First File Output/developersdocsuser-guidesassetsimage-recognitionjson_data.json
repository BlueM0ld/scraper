"Image Recognition | Amplience Developer Portal\nSkip to main contentDevelopersHOMEDOCSGet startedConceptsSchema referenceTechnologiesAPIsIntegrations and extensionsDeveloper toolsRelease notesKnowledge centerSupportStatusUser guidesSchema examplesGuides and tutorialsSUPPORTCustomer log inLightDarkSearchBack to topBack to docsUser guidesGetting to know AmplienceProducing contentScheduling contentManaging assetsUploading assetsLocating assetsWorking with assetsCaching and purgingWorking with videoPoint of InterestImage RecognitionManaging accountsSingle sign-onIntegrationsDocsUser GuidesManaging assetsImage RecognitionOn this pageImage RecognitionThe image recognition service that makes it possible to identify objects, faces and text within images uploaded to Content Hub. When the service is enabled on your account, images uploaded to Content Hub are enriched with metadata that can be used to search and filter your assets. The service uses machine learning technology to identify objects, faces and text and this information is added as image metadata.On this page we'll provide an overview of the image recognition service, show what kind of information is detected and how to use the metadata to search for images in Content Hub.OverviewLink copied!Images are usually uploaded to Content Hub using bulk upload scripts and will often have filenames consisting of just a product code, for example, making them difficult to search for. Image recognition makes it easier to search for and organise images using the metadata about the image itself, rather than just its name.You can use the image recognition metadata to search for images containing particular objects such as bags and shoes, or models within a particular age range. You can even filter out unsuitable images, something that is especially useful when used together with our User Generated Content service. You can also search for images in the Dynamic Content media browser using the image recognition metadata.Image recognition enables all kinds of possibilities when used with features such as point of interest, allowing you to detect and focus an image around a particular product.In order to use the image recognition service, it must be enabled on your Content Hub account. Contact your Customer Success Manager to request to have the service set up. The service can be enabled on one or more of your asset stores.Viewing image recognition metadata in Content HubLink copied!When an image is uploaded to Content Hub, a request is sent to the image recognition service to analyse the image. It will return information about the image that will then be stored in the image metadata. The service will detect four types of data: objects, faces, text and unsafe content and this will correspond to the \"Detected objects\", \"Detected text\", \"Detected Faces\" and \"Unsafe Content\" sections displayed when viewing the metadata in Content Hub.For each section there are two fields: Data and Labels. \"Data\" contains all the information returned by the image recognition service, while \"Labels\" contain the text that can be used for searching and filtering. In the woman with red beret image shown below, the service has detected objects classified as apparel, skirt, bag and handbag and we can search and filter on these labels.The data returned includes a confidence level, for example if someone is wearing sunglasses or if a detected object is a bag, and we only include labels for those objects and faces that have a high confidence level value.You can also retrieve the metadata using the metadata API as explained in the retrieving the full metadata section.Detected facesLink copied!If there are images detected in the image, this information will be added to the metadata, with an entry for each face detected.In the image below there are two faces detected and labels are added including information such as the estimated age range, facial expression and whether someone is wearing sunglasses. In this example two faces have been detected, so there are two \"Detected Faces\" sections.Filtering and searching using recognition metadataLink copied!In Content Hub you can choose to filter using detected objects, faces, text and unsuitable images. In this example we've chosen Detected Objects as a filter. The menu is populated with the labels that have been added to the metadata of each image that has been processed with the image recognition service.From the list of labels we choose \"purse\".With the \"purse\" filter is applied, only those images that contain a purse are displayed.We can also search within our filtered content. So if we just want those images containing bags with a tartan pattern, we can just type \"tartan\" in the search box and only the images containing this label will be displayed. In this case it's just a single image.Detecting text in an imageLink copied!The image recognition service also detects text that is part of an image. In this example the image shows a woman walking with an umbrella that has some text emblazoned on it- \"#rain\". The service detects the text and it is added as a label to the \"Detected Text\" section of the image metadata. We can then search for this text.Retrieving the full metadataLink copied!You can retrieve the full information returned by the image recognition service by using the metadata API and adding .json?metadata=true to the end of the image URL. Note that you need to ensure that the image recognition metadata data schema is made publishable in order to be able to retrieve the metadata using this approach. Your Customer Success Manager will be able to arrange this with our provisioning team.The full JSON is also shown in the 'data' field of the metadata pane in Content Hub.Here's an example of the full metadata returned for the woman in a red beret image. Retrieving the full metadata is useful if you want to know the position of an object, to use it with point of interest, for example.{\n\"isImage\":true,\n\"alpha\":false,\n\"width\":3065,\n\"height\":4598,\n\"format\":\"JPEG\",\n\"metadata\":{\n\"detectedText\":{\n\"data\":null,\n\"textLines\":null,\n\"id\":\"5d13acdb-2b52-4d32-b7d7-2b84b26aa319\"\n},\n\"detectedFaces\":{\n\"data\":{\n\"ageRange\":{\n\"high\":36,\n\"low\":19\n},\n\"boundingBox\":{\n\"top\":0.092249945,\n\"left\":0.41700655,\n\"width\":0.12484362,\n\"height\":0.13042621\n},\n\"sunglasses\":{\n\"confidence\":100,\n\"value\":false\n},\n\"mouthOpen\":{\n\"confidence\":59.774826,\n\"value\":false\n},\n\"emotions\":[\n{\n\"confidence\":0.5497097,\n\"type\":\"ANGRY\"\n},\n{\n\"confidence\":1.3447995,\n\"type\":\"HAPPY\"\n},\n{\n\"confidence\":52.429367,\n\"type\":\"SURPRISED\"\n},\n{\n\"confidence\":10.905305,\n\"type\":\"SAD\"\n},\n{\n\"confidence\":0.16092487,\n\"type\":\"DISGUSTED\"\n},\n{\n\"confidence\":0,\n\"type\":\"CONFUSED\"\n},\n{\n\"confidence\":26.975567,\n\"type\":\"CALM\"\n}\n],\n\"gender\":{\n\"confidence\":97.6458,\n\"value\":\"Female\"\n},\n\"beard\":{\n\"confidence\":99.86307,\n\"value\":false\n},\n\"pose\":{\n\"roll\":11.668715,\n\"pitch\":6.941703,\n\"yaw\":20.281464\n},\n\"confidence\":99.999985,\n\"landmarks\":[\n{\n\"x\":0.46199894,\n\"y\":0.14075498,\n\"type\":\"eyeLeft\"\n},\n{\n\"x\":0.521149,\n\"y\":0.14828268,\n\"type\":\"eyeRight\"\n},\n{\n\"x\":0.45666224,\n\"y\":0.18921822,\n\"type\":\"mouthLeft\"\n},\n{\n\"x\":0.5050401,\n\"y\":0.19534743,\n\"type\":\"mouthRight\"\n},\n{\n\"x\":0.4942966,\n\"y\":0.16859493,\n\"type\":\"nose\"\n},\n{\n\"x\":0.43792748,\n\"y\":0.1277409,\n\"type\":\"leftEyeBrowLeft\"\n},\n{\n\"x\":0.48055947,\n\"y\":0.12784877,\n\"type\":\"leftEyeBrowRight\"\n},\n{\n\"x\":0.461382,\n\"y\":0.12387163,\n\"type\":\"leftEyeBrowUp\"\n},\n{\n\"x\":0.5155438,\n\"y\":0.13239746,\n\"type\":\"rightEyeBrowLeft\"\n},\n{\n\"x\":0.5442879,\n\"y\":0.1409053,\n\"type\":\"rightEyeBrowRight\"\n},\n{\n\"x\":0.53228,\n\"y\":0.13247447,\n\"type\":\"rightEyeBrowUp\"\n},\n{\n\"x\":0.4498693,\n\"y\":0.13957764,\n\"type\":\"leftEyeLeft\"\n},\n{\n\"x\":0.47307774,\n\"y\":0.14277685,\n\"type\":\"leftEyeRight\"\n},\n{\n\"x\":0.46232894,\n\"y\":0.138623,\n\"type\":\"leftEyeUp\"\n},\n{\n\"x\":0.46148655,\n\"y\":0.1429718,\n\"type\":\"leftEyeDown\"\n},\n{\n\"x\":0.50867873,\n\"y\":0.14721212,\n\"type\":\"rightEyeLeft\"\n},\n{\n\"x\":0.52974313,\n\"y\":0.14937758,\n\"type\":\"rightEyeRight\"\n},\n{\n\"x\":0.5215662,\n\"y\":0.14590569,\n\"type\":\"rightEyeUp\"\n},\n{\n\"x\":0.519888,\n\"y\":0.15011361,\n\"type\":\"rightEyeDown\"\n},\n{\n\"x\":0.47700205,\n\"y\":0.17342311,\n\"type\":\"noseLeft\"\n},\n{\n\"x\":0.49826175,\n\"y\":0.17563091,\n\"type\":\"noseRight\"\n},\n{\n\"x\":0.4856338,\n\"y\":0.18528166,\n\"type\":\"mouthUp\"\n},\n{\n\"x\":0.48096976,\n\"y\":0.19928539,\n\"type\":\"mouthDown\"\n},\n{\n\"x\":0.46199894,\n\"y\":0.14075498,\n\"type\":\"leftPupil\"\n},\n{\n\"x\":0.521149,\n\"y\":0.14828268,\n\"type\":\"rightPupil\"\n},\n{\n\"x\":0.40742865,\n\"y\":0.13887839,\n\"type\":\"upperJawlineLeft\"\n},\n{\n\"x\":0.41126552,\n\"y\":0.19017029,\n\"type\":\"midJawlineLeft\"\n},\n{\n\"x\":0.47100845,\n\"y\":0.2234472,\n\"type\":\"chinBottom\"\n},\n{\n\"x\":0.519395,\n\"y\":0.20340215,\n\"type\":\"midJawlineRight\"\n},\n{\n\"x\":0.5419624,\n\"y\":0.1553815,\n\"type\":\"upperJawlineRight\"\n}\n],\n\"mustache\":{\n\"confidence\":99.99686,\n\"value\":false\n},\n\"smile\":{\n\"confidence\":97.04133,\n\"value\":false\n},\n\"quality\":{\n\"brightness\":70.10285,\n\"sharpness\":86.86019\n},\n\"eyesOpen\":{\n\"confidence\":97.59302,\n\"value\":false\n},\n\"eyeglasses\":{\n\"confidence\":99.99999,\n\"value\":false\n}\n},\n\"faceId\":\"5d13acdb-2b52-4d32-b7d7-2b84b26aa319_0\",\n\"labels\":[\n\"20-40\",\n\"female\",\n\"eyes-closed\",\n\"mouth-closed\"\n]\n},\n\"detectedObjects\":{\n\"data\":[\n{\n\"instances\":null,\n\"confidence\":99.79362,\n\"name\":\"Apparel\",\n\"parents\":null\n},\n{\n\"instances\":[\n{\n\"boundingBox\":{\n\"top\":0.4801911,\n\"left\":0.28257713,\n\"width\":0.49396697,\n\"height\":0.42884958\n},\n\"confidence\":99.79362\n}\n],\n\"confidence\":99.79362,\n\"name\":\"Skirt\",\n\"parents\":[\n{\n\"name\":\"Clothing\"\n}\n]\n},\n{\n\"instances\":null,\n\"confidence\":99.79362,\n\"name\":\"Clothing\",\n\"parents\":null\n},\n{\n\"instances\":null,\n\"confidence\":92.96846,\n\"name\":\"Accessories\",\n\"parents\":null\n},\n{\n\"instances\":null,\n\"confidence\":92.96846,\n\"name\":\"Accessory\",\n\"parents\":null\n},\n{\n\"instances\":null,\n\"confidence\":91.75725,\n\"name\":\"Human\",\n\"parents\":null\n},\n{\n\"instances\":[\n{\n\"boundingBox\":{\n\"top\":0.044115435,\n\"left\":0.24598205,\n\"width\":0.5202878,\n\"height\":0.9390389\n},\n\"confidence\":91.75725\n}\n],\n\"confidence\":91.75725,\n\"name\":\"Person\",\n\"parents\":null\n},\n{\n\"instances\":[\n{\n\"boundingBox\":{\n\"top\":0.041444357,\n\"left\":0.3652041,\n\"width\":0.25975224,\n\"height\":0.12184165\n},\n\"confidence\":86.01483\n}\n],\n\"confidence\":86.01483,\n\"name\":\"Hat\",\n\"parents\":[\n{\n\"name\":\"Clothing\"\n}\n]\n},\n{\n\"instances\":null,\n\"confidence\":85.07383,\n\"name\":\"Handbag\",\n\"parents\":[\n{\n\"name\":\"Bag\"\n},\n{\n\"name\":\"Accessories\"\n}\n]\n},\n{\n\"instances\":null,\n\"confidence\":85.07383,\n\"name\":\"Bag\",\n\"parents\":null\n},\n{\n\"instances\":null,\n\"confidence\":75.28065,\n\"name\":\"Armor\",\n\"parents\":null\n},\n{\n\"instances\":null,\n\"confidence\":75.28065,\n\"name\":\"Chain Mail\",\n\"parents\":[\n{\n\"name\":\"Armor\"\n}\n]\n},\n{\n\"instances\":null,\n\"confidence\":72.7458,\n\"name\":\"Purse\",\n\"parents\":[\n{\n\"name\":\"Handbag\"\n},\n{\n\"name\":\"Bag\"\n},\n{\n\"name\":\"Accessories\"\n}\n]\n},\n{\n\"instances\":null,\n\"confidence\":70.483055,\n\"name\":\"Sleeve\",\n\"parents\":[\n{\n\"name\":\"Clothing\"\n}\n]\n},\n{\n\"instances\":null,\n\"confidence\":64.96331,\n\"name\":\"Coat\",\n\"parents\":[\n{\n\"name\":\"Clothing\"\n}\n]\n},\n{\n\"instances\":null,\n\"confidence\":64.96331,\n\"name\":\"Overcoat\",\n\"parents\":[\n{\n\"name\":\"Coat\"\n},\n{\n\"name\":\"Clothing\"\n}\n]\n}\n],\n\"id\":\"5d13acdb-2b52-4d32-b7d7-2b84b26aa319\",\n\"labels\":[\n\"apparel\",\n\"skirt\",\n\"clothing\",\n\"accessories\",\n\"accessory\",\n\"human\",\n\"person\",\n\"hat\",\n\"handbag\",\n\"bag\",\n\"armor\",\n\"chain mail\",\n\"purse\",\n\"sleeve\",\n\"coat\",\n\"overcoat\"\n]\n},\n\"detectedUnsafeContent\":{\n\"data\":null,\n\"id\":\"5d13acdb-2b52-4d32-b7d7-2b84b26aa319\",\n\"labels\":null\n}\n},\n\"status\":\"ok\"}Related pagesLink copied!Metadata page on the Dynamic Media playgroundPoint of interestPreviousPoint of InterestNextManaging accountsOverviewViewing image recognition metadata in Content HubDetected facesFiltering and searching using recognition metadataDetecting text in an imageRetrieving the full metadataRelated pagesProductsWhy AmplienceDynamic ContentContent HubDynamic MediaIntegration MarketplaceMACH AllianceRequest TrialProduct StatusResourcesBlogDocumentationCompareReports & WhitepapersWebinars & VideosPlaygroundSupportRegister a DealKey conceptsHeadless CMSHeadless CommerceComposable CommerceAgile CMSJavascript CMSReact CMSNext.js CMSJamstack CMSCompanyAboutContactCustomersPartnersCareersTerms & ConditionsPrivacy PolicyCookie PolicyHeadless Content and Commerce Simplified\u00a9 2023 Amplience. All rights reserved."